{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Modelo Alternativo 2: Support Vector Machine (SVM)\n",
        "\n",
        "## Descripción\n",
        "\n",
        "Este notebook presenta una **segunda aproximación alternativa** usando **Support Vector Machine** para clasificación multiclase.\n",
        "\n",
        "### ¿Por qué SVM?\n",
        "\n",
        "- **Márgenes máximos**: Busca el hiperplano óptimo que maximiza la separación entre clases\n",
        "- **Kernel trick**: Puede capturar relaciones no lineales mediante transformaciones implícitas\n",
        "- **Efectivo en alta dimensionalidad**: Funciona bien con muchas features\n",
        "- **Robusto**: Menos propenso a overfitting con regularización apropiada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Importaciones y Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Librerías cargadas correctamente\n"
          ]
        }
      ],
      "source": [
        "# Librerías básicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Modelo SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Métricas\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Configuración\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"✓ Librerías cargadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (692500, 21)\n",
            "Test: (296786, 20)\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_ids = test['ID'].copy()\n",
        "\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Test: {test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Preprocesamiento\n",
        "\n",
        "Aplicamos el mismo pipeline pero **con estandarización** ya que SVM es muy sensible a las escalas de las variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocesando train...\n",
            "\n",
            "Preprocesando test...\n",
            "\n",
            "Preprocesando test...\n",
            "\n",
            "✓ Train: (692500, 72)\n",
            "✓ Test: (296786, 71)\n",
            "\n",
            "✓ Train: (692500, 72)\n",
            "✓ Test: (296786, 71)\n"
          ]
        }
      ],
      "source": [
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"Preprocesamiento para SVM (sin estandarización, se aplica después)\"\"\"\n",
        "    df_proc = df.copy()\n",
        "    \n",
        "    # Eliminar duplicados\n",
        "    if 'F_TIENEINTERNET.1' in df_proc.columns:\n",
        "        df_proc = df_proc.drop('F_TIENEINTERNET.1', axis=1)\n",
        "    \n",
        "    # E_VALORMATRICULAUNIVERSIDAD\n",
        "    def convertir_valor_matricula(valor):\n",
        "        if pd.isna(valor): return np.nan\n",
        "        elif 'Menos de 500 mil' in valor: return 250000\n",
        "        elif 'Entre 500 mil y menos de 1 millón' in valor: return 750000\n",
        "        elif 'Entre 1 millón y menos de 2.5 millones' in valor: return 1750000\n",
        "        elif 'Entre 2.5 millones y menos de 4 millones' in valor: return 3250000\n",
        "        elif 'Entre 4 millones y menos de 5.5 millones' in valor: return 4750000\n",
        "        elif 'Entre 5.5 millones y menos de 7 millones' in valor: return 6250000\n",
        "        elif 'Más de 7 millones' in valor: return 7500000\n",
        "        elif 'No pagó matrícula' in valor: return 0\n",
        "        else: return np.nan\n",
        "    \n",
        "    if 'E_VALORMATRICULAUNIVERSIDAD' in df_proc.columns:\n",
        "        df_proc['E_VALORMATRICULAUNIVERSIDAD'] = df_proc['E_VALORMATRICULAUNIVERSIDAD'].apply(convertir_valor_matricula)\n",
        "        df_proc['E_VALORMATRICULAUNIVERSIDAD'].fillna(df_proc['E_VALORMATRICULAUNIVERSIDAD'].mean(), inplace=True)\n",
        "    \n",
        "    # E_HORASSEMANATRABAJA\n",
        "    def convertir_horas_trabajadas(valor):\n",
        "        if pd.isna(valor): return np.nan\n",
        "        valor = str(valor)\n",
        "        if valor == '0': return 0\n",
        "        elif 'Menos de 10 horas' in valor: return 5\n",
        "        elif 'Entre 11 y 20 horas' in valor: return 15.5\n",
        "        elif 'Entre 21 y 30 horas' in valor: return 25.5\n",
        "        elif 'Más de 30 horas' in valor: return 35\n",
        "        return np.nan\n",
        "    \n",
        "    if 'E_HORASSEMANATRABAJA' in df_proc.columns:\n",
        "        df_proc['E_HORASSEMANATRABAJA'] = df_proc['E_HORASSEMANATRABAJA'].apply(convertir_horas_trabajadas)\n",
        "        df_proc['E_HORASSEMANATRABAJA'].fillna(0, inplace=True)\n",
        "    \n",
        "    # F_ESTRATOVIVIENDA\n",
        "    if 'F_ESTRATOVIVIENDA' in df_proc.columns:\n",
        "        estrato_map = {'Sin Estrato': 0, 'Estrato 1': 1, 'Estrato 2': 2, \n",
        "                       'Estrato 3': 3, 'Estrato 4': 4, 'Estrato 5': 5, 'Estrato 6': 6}\n",
        "        df_proc['F_ESTRATOVIVIENDA'] = df_proc['F_ESTRATOVIVIENDA'].map(estrato_map)\n",
        "        df_proc['F_ESTRATOVIVIENDA'].fillna(0, inplace=True)\n",
        "    \n",
        "    # Variables binarias\n",
        "    binary_cols = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', \n",
        "                   'F_TIENECOMPUTADOR', 'E_PRIVADO_LIBERTAD', 'E_PAGOMATRICULAPROPIO']\n",
        "    for col in binary_cols:\n",
        "        if col in df_proc.columns:\n",
        "            df_proc[col] = df_proc[col].map({'Si': 1, 'No': 0})\n",
        "            df_proc[col].fillna(0, inplace=True)\n",
        "    \n",
        "    # Target\n",
        "    if is_train and 'RENDIMIENTO_GLOBAL' in df_proc.columns:\n",
        "        target_map = {'bajo': 0, 'medio-bajo': 1, 'medio-alto': 2, 'alto': 3}\n",
        "        df_proc['RENDIMIENTO_GLOBAL'] = df_proc['RENDIMIENTO_GLOBAL'].map(target_map)\n",
        "    \n",
        "    # Educación padres - One-hot\n",
        "    def to_onehot(x):\n",
        "        values = np.unique(x)\n",
        "        indices = [np.argwhere(i == values)[0][0] for i in x]\n",
        "        return np.eye(len(values))[indices].astype(int)\n",
        "    \n",
        "    def replace_column_with_onehot(data, col):\n",
        "        values = np.unique(data[col])\n",
        "        onehot_matrix = to_onehot(data[col].values)\n",
        "        return pd.DataFrame(onehot_matrix, \n",
        "                          columns=[f\"{col}_{values[i]}\" for i in range(onehot_matrix.shape[1])], \n",
        "                          index=data.index)\n",
        "    \n",
        "    for col in ['F_EDUCACIONMADRE', 'F_EDUCACIONPADRE']:\n",
        "        if col in df_proc.columns:\n",
        "            df_proc[col].fillna('No Aplica', inplace=True)\n",
        "            onehot_df = replace_column_with_onehot(df_proc[[col]], col)\n",
        "            df_proc = df_proc.join(onehot_df)\n",
        "            df_proc.drop(col, axis=1, inplace=True)\n",
        "    \n",
        "    # E_PRGM_ACADEMICO\n",
        "    if 'E_PRGM_ACADEMICO' in df_proc.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_proc['E_PRGM_ACADEMICO'] = le.fit_transform(df_proc['E_PRGM_ACADEMICO'])\n",
        "    \n",
        "    # E_PRGM_DEPARTAMENTO\n",
        "    if 'E_PRGM_DEPARTAMENTO' in df_proc.columns:\n",
        "        df_proc['E_PRGM_DEPARTAMENTO'].fillna('DESCONOCIDO', inplace=True)\n",
        "        depto_onehot = replace_column_with_onehot(df_proc[['E_PRGM_DEPARTAMENTO']], 'E_PRGM_DEPARTAMENTO')\n",
        "        df_proc = df_proc.join(depto_onehot)\n",
        "        df_proc.drop('E_PRGM_DEPARTAMENTO', axis=1, inplace=True)\n",
        "    \n",
        "    # Indicadores\n",
        "    indicator_cols = [col for col in df_proc.columns if col.startswith('INDICADOR_')]\n",
        "    for col in indicator_cols:\n",
        "        if df_proc[col].isnull().sum() > 0:\n",
        "            df_proc[col].fillna(df_proc[col].median(), inplace=True)\n",
        "    \n",
        "    # Feature Engineering\n",
        "    recursos = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR']\n",
        "    recursos = [c for c in recursos if c in df_proc.columns]\n",
        "    if recursos:\n",
        "        df_proc['TOTAL_RECURSOS_HOGAR'] = df_proc[recursos].sum(axis=1)\n",
        "    \n",
        "    if indicator_cols:\n",
        "        df_proc['PROMEDIO_INDICADORES'] = df_proc[indicator_cols].mean(axis=1)\n",
        "    \n",
        "    # Eliminar ID y PERIODO\n",
        "    for col in ['ID', 'PERIODO_ACADEMICO']:\n",
        "        if col in df_proc.columns:\n",
        "            df_proc = df_proc.drop(col, axis=1)\n",
        "    \n",
        "    return df_proc\n",
        "\n",
        "print(\"Preprocesando train...\")\n",
        "train_processed = preprocess_data(train, is_train=True)\n",
        "print(\"\\nPreprocesando test...\")\n",
        "test_processed = preprocess_data(test, is_train=False)\n",
        "print(f\"\\n✓ Train: {train_processed.shape}\")\n",
        "print(f\"✓ Test: {test_processed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ X: (692500, 71)\n",
            "✓ X_test: (296786, 71)\n"
          ]
        }
      ],
      "source": [
        "# Separar features y target\n",
        "y = train_processed['RENDIMIENTO_GLOBAL'].copy()\n",
        "X = train_processed.drop('RENDIMIENTO_GLOBAL', axis=1)\n",
        "X_test = test_processed.copy()\n",
        "\n",
        "# Alinear columnas\n",
        "all_cols = sorted(list(set(X.columns) | set(X_test.columns)))\n",
        "X = X.reindex(columns=all_cols, fill_value=0)\n",
        "X_test = X_test.reindex(columns=all_cols, fill_value=0)\n",
        "\n",
        "print(f\"✓ X: {X.shape}\")\n",
        "print(f\"✓ X_test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. División de Datos y Estandarización\n",
        "\n",
        "**Importante**: SVM requiere estandarización para funcionar correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 554,000\n",
            "Validation: 138,500\n",
            "Features: 71\n",
            "\n",
            "✓ Datos estandarizados correctamente\n"
          ]
        }
      ],
      "source": [
        "# División de datos\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# Estandarización (CRUCIAL para SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train: {X_train_scaled.shape[0]:,}\")\n",
        "print(f\"Validation: {X_val_scaled.shape[0]:,}\")\n",
        "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"\\n✓ Datos estandarizados correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Modelo Base SVM\n",
        "\n",
        "Comenzamos con un modelo base usando kernel RBF (Radial Basis Function).\n",
        "\n",
        "> **Nota**: SVM es computacionalmente costoso, por lo que usamos una muestra para la búsqueda de hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SVM - MODELO BASE (kernel RBF)\n",
            "================================================================================\n",
            "\n",
            "Entrenando modelo base...\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SVM - MODELO BASE (kernel RBF)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Modelo base con parámetros por defecto\n",
        "svm_base = SVC(\n",
        "    kernel='rbf',\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"\\nEntrenando modelo base...\")\n",
        "svm_base.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_train_pred_base = svm_base.predict(X_train_scaled)\n",
        "y_val_pred_base = svm_base.predict(X_val_scaled)\n",
        "\n",
        "train_acc_base = accuracy_score(y_train, y_train_pred_base)\n",
        "val_acc_base = accuracy_score(y_val, y_val_pred_base)\n",
        "\n",
        "print(f\"\\nAccuracy Train: {train_acc_base:.4f}\")\n",
        "print(f\"Accuracy Validation: {val_acc_base:.4f}\")\n",
        "print(f\"Diferencia: {train_acc_base - val_acc_base:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Optimización de Hiperparámetros\n",
        "\n",
        "Usamos GridSearchCV con una muestra reducida para optimizar `C` y `gamma`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Usamos una muestra para acelerar la búsqueda\n",
        "sample_size = min(5000, len(X_train_scaled))\n",
        "sample_idx = np.random.choice(len(X_train_scaled), sample_size, replace=False)\n",
        "X_sample = X_train_scaled[sample_idx]\n",
        "y_sample = y_train.iloc[sample_idx]\n",
        "\n",
        "print(f\"\\nUsando muestra de {sample_size:,} observaciones para optimización\")\n",
        "\n",
        "# Grid de hiperparámetros\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'poly']\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=RANDOM_STATE)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nBuscando mejores hiperparámetros...\")\n",
        "grid_search.fit(X_sample, y_sample)\n",
        "\n",
        "print(\"\\n✓ Optimización completa\")\n",
        "print(f\"\\nMejores hiperparámetros:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "print(f\"\\nMejor score CV: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Modelo Final Optimizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SVM OPTIMIZADO - ENTRENAMIENTO COMPLETO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Entrenar con todos los datos usando los mejores hiperparámetros\n",
        "best_svm = SVC(\n",
        "    **grid_search.best_params_,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"\\nEntrenando modelo optimizado con todos los datos...\")\n",
        "best_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_train_pred = best_svm.predict(X_train_scaled)\n",
        "y_val_pred = best_svm.predict(X_val_scaled)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\nAccuracy Train: {train_acc:.4f}\")\n",
        "print(f\"Accuracy Validation: {val_acc:.4f}\")\n",
        "print(f\"Diferencia: {train_acc - val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nMejora vs modelo base:\")\n",
        "print(f\"  Train: {train_acc - train_acc_base:+.4f}\")\n",
        "print(f\"  Validation: {val_acc - val_acc_base:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_names = ['bajo', 'medio-bajo', 'medio-alto', 'alto']\n",
        "print(classification_report(y_val, y_val_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matriz de Confusión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Matriz de Confusión - SVM', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Real')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Accuracy por clase\n",
        "print(\"\\nAccuracy por clase:\")\n",
        "for i, clase in enumerate(target_names):\n",
        "    class_acc = cm[i, i] / cm[i, :].sum()\n",
        "    print(f\"  {clase}: {class_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Predicciones en Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicciones en test\n",
        "y_test_pred = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# Convertir a etiquetas\n",
        "target_map_inverse = {0: 'bajo', 1: 'medio-bajo', 2: 'medio-alto', 3: 'alto'}\n",
        "y_test_labels = pd.Series(y_test_pred).map(target_map_inverse)\n",
        "\n",
        "# Crear submission\n",
        "submission_svm = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'RENDIMIENTO_GLOBAL': y_test_labels\n",
        "})\n",
        "\n",
        "submission_svm.to_csv('submission_svm.csv', index=False)\n",
        "\n",
        "print(f\"✓ Predicciones generadas: {len(y_test_pred):,}\")\n",
        "print(f\"\\nDistribución en test:\")\n",
        "print(y_test_labels.value_counts().sort_index())\n",
        "print(f\"\\n✓ Archivo guardado: submission_svm.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Conclusiones\n",
        "\n",
        "### Ventajas de SVM:\n",
        "- ✅ **Márgenes óptimos**: Busca la mejor separación entre clases\n",
        "- ✅ **Kernel trick**: Captura relaciones no lineales complejas\n",
        "- ✅ **Efectivo en alta dimensionalidad**: Funciona bien con muchas features\n",
        "- ✅ **Robusto con parámetro C apropiado**: Menos overfitting\n",
        "\n",
        "### Desventajas:\n",
        "- ❌ **Muy lento**: No escala bien a datasets grandes \n",
        "- ❌ **Requiere estandarización**: Preprocessing más cuidadoso\n",
        "- ❌ **No probabilístico por defecto**: Necesita calibración para probabilidades\n",
        "- ❌ **Memoria intensiva**: Guarda vectores de soporte\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
